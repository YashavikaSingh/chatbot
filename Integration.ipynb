{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enchancing the query to get resource links from google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Using cached gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.12/site-packages (from gtts) (2.32.2)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /opt/anaconda3/lib/python3.12/site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.27->gtts) (2024.7.4)\n",
      "Using cached gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgtts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gTTS\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpresidio_analyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalyzerEngine\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydub'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pytesseract\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from flask import Flask, request\n",
    "from twilio.twiml.messaging_response import MessagingResponse\n",
    "import json\n",
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "import uuid\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "import time\n",
    "from threading import Timer\n",
    "from flask import Flask, request, Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n",
    "    api_key=\"\"\n",
    ")\n",
    "ngrokurl = \"https://4329-216-165-95-191.ngrok-free.app\"\n",
    "response = MessagingResponse()\n",
    "preferred_language = None  \n",
    "current_context=\"\"\n",
    "image_batches = {}\n",
    "# Image batch timeout (seconds)\n",
    "IMAGE_BATCH_TIMEOUT = 3  # Reduced timeout for better performance\n",
    "# Twilio API Credentials\n",
    "TWILIO_ACCOUNT_SID = 'AC979b7c1910156016bcd92fb11773217e'  # Replace with actual Account SID\n",
    "TWILIO_AUTH_TOKEN = 'b792f7ce9131d3b5fc1d2789f708bbad'  # Replace with actual Auth Token\n",
    "twilio_number = 'whatsapp:+14155238886'  # Twilio WhatsApp number\n",
    "\n",
    "# Initialize Presidio for text anonymization\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()\n",
    "# Store user sessions & batch images\n",
    "user_sessions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_query(context):\n",
    "    # Define key enhancements for a better search query\n",
    "    query_keywords = [\n",
    "        \"best practices\", \"guide\", \"overview\", \"tutorial\", \"PDF\",\n",
    "        \"site:.gov\", \"site:.edu\", \"filetype:pdf\", \"latest\", \"research\",\n",
    "        \"2024\", \"official documentation\", \"explained\", \"how to\"\n",
    "    ]\n",
    "\n",
    "    # Basic processing to remove unnecessary words\n",
    "    important_terms = [word for word in context.split() if len(word) > 3]\n",
    "    \n",
    "    # Generate a structured query by combining important terms with keywords\n",
    "    structured_query = \" \".join(important_terms[:6])  # Prioritize the first few key terms\n",
    "    structured_query += \" \" + \" OR \".join(query_keywords)  # Add relevant keywords\n",
    "\n",
    "    return structured_query\n",
    "\n",
    "\n",
    "\n",
    "def extract_resource_query(chat_message):\n",
    "    \"\"\"\n",
    "    Identifies and extracts the main resource or information request query from a chat message.\n",
    "\n",
    "    Args:\n",
    "        chat_message (str): The input chat message.\n",
    "\n",
    "    Returns:\n",
    "        str: A JSON string containing the extracted query under the key \"query\".\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an intelligent assistant. Your task is to analyze a given chat message and extract only the core resource request or information-seeking query. \n",
    "    If the message does not contain a request, return an empty query.\n",
    "    \n",
    "    Example:\n",
    "    User: \"Where can I find NYC construction permit requirements?\"\n",
    "    Output: {{\"query\": \"NYC construction permit requirements\"}}\n",
    "    \n",
    "    User: \"Tell me about machine learning models.\"\n",
    "    Output: {{\"query\": \"machine learning models\"}}\n",
    "    \n",
    "    User: \"Hey, how's your day?\"\n",
    "    Output: {{\"query\": \"\"}}\n",
    "    \n",
    "    Now, extract the query from this message:\n",
    "    \"{chat_message}\"\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    extracted_query = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "    return json.dumps({\"query\": extracted_query}, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "TEXT TO AUDIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_audio(text, filename=\"audio.mp3\"):\n",
    "    # Use gTTS to convert text to speech and save it in the 'audio_files' directory\n",
    "    ngrokurl = \"https://4329-216-165-95-191.ngrok-free.app\"\n",
    "    random_filename = f\"{uuid.uuid4().hex}.mp3\"\n",
    "    audio_path = os.path.join('audio_files', random_filename)\n",
    "    tts = gTTS(text, lang='en')\n",
    "    tts.save(audio_path)\n",
    "    media_url = f\"{ngrokurl}/audio_files/\"+random_filename\n",
    "    return media_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "Audio To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(audio_url):\n",
    "    \"\"\"Download an audio file from Twilio and transcribe it using OpenAI Whisper API.\"\"\"\n",
    "    try:\n",
    "        # Twilio requires authentication to fetch media\n",
    "        twilio_sid = os.getenv(\"ACCOUNT_SID\")  # Store these in environment variables\n",
    "        twilio_auth_token = os.getenv(\"AUTH_TOKEN\")\n",
    "\n",
    "        # Download the audio file with authentication\n",
    "        print(f\"Fetching audio from: {audio_url}\")  # Log URL for debugging\n",
    "        response = requests.get(audio_url, auth=(twilio_sid, twilio_auth_token))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to download audio. Status code: {response.status_code}, Response: {response.text}\")\n",
    "            return \"Failed to download audio.\"\n",
    "\n",
    "        # Save the audio file temporarily\n",
    "        audio_path = \"temp_audio.ogg\"\n",
    "        with open(audio_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Convert to WAV format (Whisper API prefers WAV)\n",
    "        audio = AudioSegment.from_file(audio_path)\n",
    "        wav_path = \"temp_audio.wav\"\n",
    "        audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "        # Transcribe using OpenAI Whisper\n",
    "        with open(wav_path, \"rb\") as f:\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",  # Specify the model for transcription\n",
    "                file=f,\n",
    "                response_format=\"text\"  # Get the response as plain text\n",
    "            )\n",
    "            print(transcript)\n",
    "\n",
    "        # Clean up files\n",
    "        os.remove(audio_path)\n",
    "        os.remove(wav_path)\n",
    "\n",
    "        return transcript  # The transcript is returned as plain text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in transcription: {e}\")\n",
    "        return f\"Error in transcription: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Extracting important dates and inforamtions (more information retiervival might be needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting important dates and inforamtions (more information retiervival might be needed)\n",
    "def extract_dates_and_headings_with_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "    Extract all dates and their associated headings/tasks (e.g., deadlines, due dates) from the following text. \n",
    "    Return the results in the format:\n",
    "    - Date: YYYY-MM-DD, Heading/Task: [description]\n",
    "    - Date: YYYY-MM-DD, Heading/Task: [description]\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful AI assistant that extracts dates and their associated tasks from text.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation for language\n",
    "def translate_text_with_gpt(text, target_language):\n",
    "    prompt = f\"Translate the following text into {target_language}:\\n\\n{text}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful AI assistant that translates text.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, extracted_context):\n",
    "    \"\"\"\n",
    "    Classifies a given user query into predefined categories based on the extracted document context.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The user query to classify.\n",
    "        extracted_context (str): The extracted text from the document.\n",
    "\n",
    "    Returns:\n",
    "        dict: A JSON response containing category classification, explanation, and preferred language (if applicable).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies user queries based on the given context and predefined categories. Return the output in JSON format.\"},\n",
    "        \n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Context:  \n",
    "{extracted_context}\n",
    "\n",
    "Task:  \n",
    "Classify the user's query into one or more of the following categories:\n",
    "\n",
    "Categories:  \n",
    "1. Document Analysis – The user wants to analyze or interpret the document's content.  \n",
    "   Example: \"What does this document mean?\"  \n",
    "2. Information Request – The user is asking for more resources or related information.  \n",
    "   Example: \"Where can I learn more about this?\"  \n",
    "3. Community Assistance – The user is looking for support from a group or housing/community help.  \n",
    "   Example: \"Where can I find affordable housing?\"  \n",
    "4. Language Translation – The user requests translation of text into another language.  \n",
    "   Example: \"Can you translate this into Spanish?\"  \n",
    "5. Summarization – The user asks for a brief summary of the document.  \n",
    "   Example: \"Summarize this document for me.\"  \n",
    "6. Document Identification – The user asks what kind of document this is.  \n",
    "   Example: \"Is this a legal document?\"  \n",
    "7. Contextual Question – The user asks a question directly related to the extracted context.  \n",
    "   Example: \"When was this law passed?\"  \n",
    "8. Audio Request – The user wants to hear information or requests audio translation.  \n",
    "   Example: \"Can you read this aloud?\" or \"I need this in spoken French.\"  \n",
    "9. Preferred Language Setting – The user explicitly states a language for all future responses.  \n",
    "   Example: \"Always translate to German.\"  \n",
    "10. Other/Unprocessable – The query doesn’t fit into the categories or is unclear.  \n",
    "   Example: \"Hi, how are you?\"  \n",
    "\n",
    "Few-Shot Examples:\n",
    "\n",
    "Example 1 (Single Category):  \n",
    "User Query: \"Can you translate this text into German?\"  \n",
    "Classification Output:  \n",
    "{{\"categories\": [\"Language Translation\"], \"language\": \"German\", \"explanation\": \"User is requesting a translation of the document into German.\"}}  \n",
    "\n",
    "Example 2 (Multiple Categories):  \n",
    "User Query: \"I need help finding an affordable apartment. Can you translate this information into Tamil?\"  \n",
    "Classification Output:  \n",
    "{{\"categories\": [\"Community Assistance\", \"Language Translation\"], \"language\": \"Tamil\", \"explanation\": \"The user is looking for housing assistance and also requesting translation into Tamil.\"}}  \n",
    "\n",
    "Example 3 (Audio Request):  \n",
    "User Query: \"Can you read this text aloud?\"  \n",
    "Classification Output:  \n",
    "{{\"categories\": [\"Audio Request\"], \"explanation\": \"User is requesting spoken audio output of the document.\"}}  \n",
    "\n",
    "Example 4 (Preferred Language Setting):  \n",
    "User Query: \"Always translate everything into French.\"  \n",
    "Classification Output:  \n",
    "{{\"categories\": [\"Preferred Language Setting\"], \"language\": \"French\", \"explanation\": \"User wants all translations to be done in French from now on.\"}}  \n",
    "\n",
    "Now Classify the Following User Query:  \n",
    "User Query: {text}  \n",
    "\n",
    "Return a JSON object in this format:  \n",
    "{{  \n",
    "    \"categories\": [\"Category1\", \"Category2\"],  \n",
    "    \"language\": \"Detected Language (if applicable)\",  \n",
    "    \"explanation\": \"Brief reason for each category classification.\"  \n",
    "}}\n",
    "\"\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = json.loads(response.choices[0].message.content.strip())\n",
    "    except json.JSONDecodeError:\n",
    "        result = {\"categories\": [\"Other/Unprocessable\"], \"language\": None, \"explanation\": \"The query could not be processed.\"}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Resource part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_custom_search(query, start=1, num_results=10):\n",
    "        print(f\"Fetching results {start} to {start + num_results - 1} for query: {query}\")\n",
    "        url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "        params = {\n",
    "            \"key\": \"AIzaSyCqRlP1kqDSj6A3-NLhcRnrrLE_KmP8nKo\",\n",
    "            \"cx\": \"63cb67bae11e44d04\",\n",
    "            \"q\": query,\n",
    "            \"start\": start,\n",
    "            \"num\": num_results\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return [item.get(\"link\", \"\") for item in data.get(\"items\", [])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senesitive part removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_text(text):\n",
    "    \"\"\"Anonymizes sensitive data using Presidio.\"\"\"\n",
    "    try:\n",
    "        results = analyzer.analyze(text=text, language=\"en\")\n",
    "        return anonymizer.anonymize(text=text, analyzer_results=results).text\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Anonymization Error: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image_url):\n",
    "    \"\"\"Downloads an image from Twilio and processes it.\"\"\"\n",
    "    try:\n",
    "        print(f\"📷 Downloading image from: {image_url}\")\n",
    "        response = requests.get(image_url, auth=(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN))\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ Failed to download image. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        # Open and convert image to grayscale\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"L\")\n",
    "        valid_formats = [\"JPEG\", \"PNG\", \"BMP\", \"GIF\", \"TIFF\", \"WEBP\"]\n",
    "        if image.format not in valid_formats:\n",
    "            print(f\"❌ Unsupported image format: {image.format}\")\n",
    "            return None\n",
    "        return image.convert(\"L\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Image Processing Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Image Processing Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_from_images(media_urls):\n",
    "    \"\"\"Processes multiple images, extracts, and combines text as one document.\"\"\"\n",
    "    extracted_texts = []\n",
    "\n",
    "    for url in media_urls:\n",
    "        image = download_image(url)\n",
    "        if image:\n",
    "            text = pytesseract.image_to_string(image, lang=\"eng\", config=\"--psm 6\").strip()\n",
    "            print(f\"📝 Extracted Text: {text[:300]}\")  # Debugging: Print first 300 characters\n",
    "            if text:\n",
    "                extracted_texts.append(text)\n",
    "\n",
    "    return \"\\n\".join(extracted_texts) if extracted_texts else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twilio Message sent back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_twilio_message(to, message):\n",
    "    \"\"\"Sends a WhatsApp message via Twilio API, splitting it if it exceeds the character limit.\"\"\"\n",
    "    max_length = 1600  # Twilio's character limit per message\n",
    "    messages = [message[i:i + max_length] for i in range(0, len(message), max_length)]\n",
    "\n",
    "    for msg in messages:\n",
    "        requests.post(\n",
    "            f\"https://api.twilio.com/2010-04-01/Accounts/{TWILIO_ACCOUNT_SID}/Messages.json\",\n",
    "            auth=(TWILIO_ACCOUNT_SID, TWILIO_AUTH_TOKEN),\n",
    "            data={\"From\": twilio_number, \"To\": to, \"Body\": msg}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    prompt = f\"Summarize this text in simple language:\\n\\n{text}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"\"\"Summarize the following text accurately. **Extract key details, names, dates, numbers, and important points.**\n",
    "        **If there is any content, provide a summary. Do not respond with 'Not mentioned' or refuse to summarize.**\"\"\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_batch(from_number):\n",
    "    \"\"\"Processes all images in a batch and sends the summary.\"\"\"\n",
    "    if from_number not in image_batches:\n",
    "        return\n",
    "\n",
    "    media_urls = image_batches.pop(from_number)[\"images\"]\n",
    "    extracted_text = extract_text_from_images(media_urls)\n",
    "\n",
    "    if not extracted_text:\n",
    "        send_twilio_message(from_number, \"❌ Could not extract text from the images. Please try again.\")\n",
    "        return\n",
    "\n",
    "    # ✅ **Ensure all images are processed as one document**\n",
    "    combined_text = anonymize_text(extracted_text)\n",
    "    summary = summarize_text(combined_text)\n",
    "\n",
    "    # Store extracted text in user session\n",
    "    user_sessions[from_number] = combined_text\n",
    "\n",
    "    # ✅ **Send only one message with all images summarized together**\n",
    "    send_twilio_message(from_number, f\"🔎 Summary (All Images Combined):\\n{summary}\\n\\n❓ What do you want to know from this?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the categories of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to execute tasks based on identified categories\n",
    "def process_categories(classification_result, user_input):\n",
    "    global preferred_language\n",
    "    response_Message = user_input\n",
    "    response = MessagingResponse()\n",
    "    categories = classification_result.get(\"categories\", [])\n",
    "    detected_language = classification_result.get(\"language\", None)\n",
    "\n",
    "    if \"Preferred Language Setting\" in categories:\n",
    "        preferred_language = detected_language  # Store user's preferred language\n",
    "        print(f\"Preferred language set to: {preferred_language}\")\n",
    "\n",
    "    if \"Summarization\" in categories:\n",
    "        print(f\"Generating summary for: {user_input}\")\n",
    "\n",
    "    if \"Document Identification\" in categories:\n",
    "        print(f\"Identifying document type for: {user_input}\")\n",
    "\n",
    "    if \"Contextual Question\" in categories:\n",
    "        print(f\"Answering question based on context: {user_input}\")\n",
    "\n",
    "    if \"Community Assistance\" in categories:\n",
    "        pass\n",
    "\n",
    "    if \"Information Request\" in categories:\n",
    "        information_query = extract_resource_query(user_input)\n",
    "        search_query_for_information_retreival = generate_search_query(information_query)\n",
    "        search_result = google_custom_search(search_query_for_information_retreival,num_results=5)\n",
    "        print(f\"Fetching additional resources related to: {user_input}\")\n",
    "        response_Message = \" The Resource links are given below \\n\"+ \"\\n\".join(search_result)\n",
    "        response.message(response_Message)\n",
    "        return response_Message\n",
    "\n",
    "    if \"Language Translation\" in categories:\n",
    "        target_lang = preferred_language if preferred_language else detected_language\n",
    "        response_Message = translate_text_with_gpt(response_Message,target_language=target_lang)\n",
    "        print(f\"Translating text to {target_lang}: {user_input}\")\n",
    "\n",
    "    if \"Audio Request\" in categories:\n",
    "        url = text_to_audio(response_Message)\n",
    "        response.message().media(url)\n",
    "        print(f\"Generating audio output for: {user_input}\")\n",
    "\n",
    "    response.message(response_Message)\n",
    "\n",
    "    return response_Message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THe code begins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"I found a document that seems to contain legal terms about rental agreements, but I’m not sure if it’s an actual contract. Also, could you summarize its key points and let me know if I need additional resources to understand it\"\n",
    "\n",
    "@app.route(\"/webhook\", methods=[\"POST\"])\n",
    "def webhook():\n",
    "    incoming_msg = request.form.get(\"Body\", \"\").strip().lower()\n",
    "    sender = request.form.get(\"From\")\n",
    "    from_number = request.form.get(\"From\")\n",
    "    num_media = int(request.form.get(\"NumMedia\", 0))\n",
    "    media_url = request.form.get(\"MediaUrl0\")  # First media file (audio/image)\n",
    "    media_type = request.form.get(\"MediaContentType0\", \"\").lower()  # Media type (audio/image)\n",
    "\n",
    "    # 🖼️ If images are sent, store them in a batch before processing\n",
    "    if num_media > 0:\n",
    "        media_urls = [request.form.get(f\"MediaUrl{i}\") for i in range(num_media)]\n",
    "        print(f\"🖼️ Number of Media Files: {num_media}\")\n",
    "\n",
    "        # Store images in batch\n",
    "        if from_number not in image_batches:\n",
    "            image_batches[from_number] = {\"images\": [], \"timestamp\": time.time()}\n",
    "\n",
    "        image_batches[from_number][\"images\"].extend(media_urls)\n",
    "\n",
    "        # ✅ **Process all images together after timeout**\n",
    "        Timer(IMAGE_BATCH_TIMEOUT, process_image_batch, args=[from_number]).start()\n",
    "\n",
    "        return Response(\"\", status=200)\n",
    "    \n",
    "    # First , Process audio if received (like audiio input)\n",
    "    if media_url and media_type.startswith(\"audio\"):\n",
    "        print(f\"Received audio URL: {media_url}\")\n",
    "\n",
    "        try:\n",
    "            # Transcribe audio\n",
    "            transcription = audio_to_text(media_url)\n",
    "            print(f\"Transcription: {transcription}\")\n",
    "\n",
    "            incoming_msg = transcription\n",
    "        except Exception as e:\n",
    "            print(f\"Error transcribing audio: {e}\")\n",
    "            response.message(\"❌ Failed to transcribe audio. Please try again.\")\n",
    "            return str(response)\n",
    "    \n",
    "    # Second Pasing the query and context to Classification part\n",
    "    classification = classify_text(incoming_msg,current_context)\n",
    "\n",
    "    # Pass the clasiification to run correcsponding functions\n",
    "    updated_response = process_categories(classification, incoming_msg)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
